# üöÄ Proxmox VE High-Availability Cluster & Ceph on OVHcloud

Bienvenue sur le d√©p√¥t de mon projet d'infrastructure hyper-converg√©e (HCI). 

Ce projet a pour objectif de d√©ployer un cluster Proxmox VE hautement disponible (HA) sur des serveurs d√©di√©s OVHcloud, en utilisant Ceph pour le stockage distribu√© r√©pliqu√©, avec une architecture hybride NVMe/HDD.

## üåü Points Cl√©s du Projet

* **Haute Disponibilit√© (HA) :** Basculement automatique (Failover) des machines virtuelles en moins de 2 minutes en cas de perte totale d'un serveur physique (Fencing via Watchdog).
* **Stockage Ceph Hybride (Tiering) :** Optimisation des performances sur des disques m√©caniques capacitifs. L'OSD Ceph stocke les donn√©es sur un RAID 5 HDD (via `mdadm` + LVM), mais d√©l√®gue sa base de donn√©es interne (DB/WAL BlueStore) sur un disque **NVMe d√©di√©** pour absorber les I/O al√©atoires.
* **S√©curit√© & Isolation :** Utilisation de l'Edge Network Firewall d'OVH en p√©riph√©rie (Default DROP). Les flux critiques de synchronisation (Corosync) et de stockage (Ceph) sont totalement isol√©s sur un r√©seau priv√© **vRack** (10 Gbps / MTU 9000).

## üõ†Ô∏è Stack Technique

* **Hyperviseur :** Proxmox VE 8 (Debian)
* **Stockage Distribu√© :** Ceph (BlueStore)
* **R√©seau :** OVH vRack (R√©seau priv√© L2)
* **Syst√®me / Disques :** LVM, mdadm (RAID 5 Logiciel)

## üìñ Documentation D√©taill√©e

Toute la d√©marche technique, l'architecture d√©taill√©e, la configuration r√©seau et les commandes de d√©ploiement sont document√©es dans le fichier principal :

üëâ **[Consulter la documentation compl√®te (DOCUMENTATION.md)](./DOCUMENTATION.md)**

---
*Projet r√©alis√© dans le cadre d'un d√©ploiement d'infrastructure de production.*
