# üöÄ D√©ploiement d'un Cluster Proxmox VE Haute Disponibilit√© (HA) & Ceph sur OVHcloud

## üìå 1. Contexte et Objectifs
L'objectif de ce projet est de d√©ployer une infrastructure hyper-converg√©e offrant une **Haute Disponibilit√© (HA)** des machines virtuelles. En cas de panne mat√©rielle d'un n≈ìud, les instances h√©berg√©es red√©marrent automatiquement sur les n≈ìuds sains, sans intervention humaine ni perte de donn√©es, gr√¢ce au stockage distribu√© Ceph.

## üñ•Ô∏è 2. Inventaire de l'Infrastructure

L'infrastructure repose sur un cluster de 3 serveurs d√©di√©s (Bare Metal).

| N≈ìud | R√¥le | IP Publique | IP Priv√©e (vRack) | Utilisateur |
| :--- | :--- | :--- | :--- | :--- |
| **Node-01** | Master / OSD / Mon | `<IP_PUB_NODE_1>` | `10.0.0.1` | `root` |
| **Node-02** | H√¥te / OSD / Mon | `<IP_PUB_NODE_2>` | `10.0.0.2` | `root` |
| **Node-03** | H√¥te / OSD / Mon | `<IP_PUB_NODE_3>` | `10.0.0.3` | `root` |

### Architecture de Stockage (Identique sur chaque Node)
Pour optimiser la volum√©trie tout en maintenant des performances IOPS √©lev√©es sur des disques m√©caniques, une **architecture hybride NVMe/HDD** a √©t√© d√©ploy√©e :
* `/dev/nvme0n1` (512 Go) ‚ûî OS Proxmox VE.
* `/dev/nvme1n1` (512 Go) ‚ûî 100% d√©di√© au cache Ceph (DB/WAL BlueStore) pour acc√©l√©rer les √©critures.
* `/dev/sda, sdb, sdc, sdd` (4x 6 To HDD) ‚ûî RAID 5 Logiciel (`mdadm`) ‚ûî Forme 1 unique OSD Ceph par serveur.
* **Capacit√© totale du cluster :** ~51 TiB brut / ~17 TiB utilisable (R√©plication x3).

---

## üîí 3. Pr√©requis et S√©curit√©

1. **D√©sactivation du Monitoring :** Configur√© sur "sans intervention proactive" c√¥t√© h√©bergeur pour permettre les tests de bascule (Crash-tests) sans d√©clencher d'incidents mat√©riels.
2. **Edge Network Firewall :** Bloqu√© par d√©faut (DROP). Seuls les acc√®s SSH administrateurs (port 22) et l'acc√®s GUI (port 8006) sont autoris√©s en entr√©e publique. Les flux inter-n≈ìuds sont whitelist√©s sur toutes les interfaces.

*Exemple de r√®gles appliqu√©es (Node-01) :*
| Priorit√© | Action | Protocole | IP Source | Port Dest. | Usage |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 0 | Autoriser | TCP | `0.0.0.0/0` | `8006` | Interface GUI Proxmox |
| 1-4 | Autoriser | TCP | `<IPs_ADMIN>` | `22` | Acc√®s SSH Administrateurs |
| 5-6 | Autoriser | IPv4 | `<IP_NODE_02_&_03>` | *any* | Inter-n≈ìuds (Cluster/Ceph) |
| 19 | Refuser | IPv4 | `0.0.0.0/0` | *any* | **DROP TOTAL** |

---

## üåê 4. Configuration R√©seau Priv√© (vRack)

Un r√©seau priv√© virtuel (vRack) isole le trafic de synchronisation du cluster (Corosync) et la r√©plication des donn√©es (Ceph). Le **MTU est fix√© √† 9000** (Jumbo Frames) pour maximiser les performances de stockage.

*Fichier `/etc/network/interfaces` :*
```bash
auto enp1s0f1
iface enp1s0f1 manual
    mtu 9000

auto vmbr1
iface vmbr1 inet static
    address 10.0.0.X/24
    bridge-ports enp1s0f1
    bridge-stp off
    bridge-fd 0
    mtu 9000
```

üîó 5. Cr√©ation du Cluster Proxmox

Cr√©ation du cluster (cluster-pve-ha) via l'interface GUI en utilisant exclusivement les adresses IP priv√©es (R√©seau 10.0.0.0/24) pour le Cluster Network (Link 0) afin de garantir la stabilit√© de Corosync face aux perturbations du r√©seau public.
üíΩ 6. Impl√©mentation du Stockage Ceph (Hybride RAID5/NVMe)
6.1. Initialisation Ceph

Installation des paquets sur les 3 n≈ìuds (R√©seaux Public/Cluster : 10.0.0.0/24). Cr√©ation des Monitors (Mon) et Managers (Mgr) pour assurer le Quorum.
6.2. Pr√©paration du RAID 5 Logiciel (mdadm)

L'agr√©gation des disques m√©caniques (HDD) se fait via mdadm sur chaque serveur :
Bash

apt update && apt install mdadm -y
mdadm --create /dev/md0 --level=5 --raid-devices=4 /dev/sda /dev/sdb /dev/sdc /dev/sdd

# Persistance au boot
mdadm --detail --scan >> /etc/mdadm/mdadm.conf
update-initramfs -u

6.3. Authentification Ceph-Volume
G√©n√©ration du Keyring autorisant la cr√©ation manuelle des OSD :
Bash

mkdir -p /var/lib/ceph/bootstrap-osd
ceph auth get client.bootstrap-osd > /var/lib/ceph/bootstrap-osd/ceph.keyring
ceph auth get client.bootstrap-osd > /etc/pve/priv/ceph.client.bootstrap-osd.keyring

6.4. Cr√©ation des OSD (LVM + Cache NVMe BlueStore)

ceph-volume n√©cessitant une couche logique pour exploiter un volume RAID logiciel, un espace LVM est cr√©√©. Le second disque NVMe (nvme1n1) y est rattach√© en tant que Block DB pour absorber les √©critures al√©atoires et compenser la latence du RAID 5 HDD :
Bash

# Cr√©ation de la couche LVM sur le RAID 5
pvcreate /dev/md0
vgcreate ceph-raid /dev/md0
lvcreate -l 100%FREE -n osd ceph-raid

# Instanciation de l'OSD avec acc√©l√©ration NVMe
ceph-volume lvm create --data ceph-raid/osd --block.db /dev/nvme1n1

6.5. Cr√©ation du Pool de R√©plication

Cr√©ation du pool Stockage-HA (Size: 3, Min Size: 2, PG Autoscale: On) et activation de l'option "Add as Storage" dans Proxmox.
üõ°Ô∏è 7. Mise en place de la Haute Disponibilit√© (HA)

    Cr√©ation du Groupe HA : Ajout des 3 n≈ìuds dans un groupe d√©fini (Cluster-Prod).

    Protection d'une VM : Assignation d'une machine virtuelle de test (dont le disque virtuel est stock√© sur Stockage-HA) √† ce groupe pour activer le Fencing.

‚úÖ 8. Validation et √âtat Cible
8.1. V√©rifications de l'√©tat du cluster
Bash

ceph -s              # Sant√© globale (Doit retourner HEALTH_OK)
cat /proc/mdstat     # √âtat de synchronisation du RAID 5 (State: clean [UUUU])
pvesm status         # Validit√© des stockages Proxmox

8.2. Crash Test (Basculement HA)

Simulation d'une perte de n≈ìud brutale (Kernel Panic d√©clench√© via echo c > /proc/sysrq-trigger).

    R√©sultat : Le Watchdog mat√©riel isole le n≈ìud d√©faillant. Le gestionnaire HA d√©tecte l'anomalie, attend l'expiration du verrou de s√©curit√©, et red√©marre automatiquement la machine virtuelle sur un n≈ìud sain en moins de 2 minutes. Validation totale du PoC.

üó∫Ô∏è 9. Sch√©ma d'Architecture Logique

```mermaid
flowchart TD
    classDef netPublic fill:#e1f5fe,stroke:#0288d1,stroke-width:2px,color:#000
    classDef netPrivate fill:#fff3e0,stroke:#e65100,stroke-width:2px,stroke-dasharray: 5 5,color:#000
    classDef pveNode fill:#f5f5f5,stroke:#333,stroke-width:2px,color:#000
    classDef osNode fill:#c8e6c9,stroke:#388e3c,color:#000
    classDef cephCache fill:#ffecb3,stroke:#ff6f00,color:#000
    classDef cephData fill:#ffccbc,stroke:#bf360c,color:#000

    subgraph Internet ["üåç Acc√®s Internet & S√©curit√© (Edge Network)"]
        FW["Pare-feu Filtrant<br>(Whitelist SSH/GUI)"]:::netPublic
    end

    subgraph Proxmox ["üñ•Ô∏è Cluster Proxmox HA"]
        N1["Node-01<br>IP Pub: <IP_PUB_1>"]:::pveNode
        N2["Node-02<br>IP Pub: <IP_PUB_2>"]:::pveNode
        N3["Node-03<br>IP Pub: <IP_PUB_3>"]:::pveNode
    end

    FW ==> N1
    FW ==> N2
    FW ==> N3

    subgraph vRack ["üîí R√©seau Priv√© (vRack)"]
        Switch{"Switch vRack<br>10 Gbps / MTU 9000"}:::netPrivate
    end

    N1 -.->|"IP: 10.0.0.1<br>(Corosync + Ceph)"| Switch
    N2 -.->|"IP: 10.0.0.2<br>(Corosync + Ceph)"| Switch
    N3 -.->|"IP: 10.0.0.3<br>(Corosync + Ceph)"| Switch

    subgraph Stockage ["üíΩ Architecture Stockage Hybride"]
        
        subgraph S1 ["Stockage Node-01"]
            OS1["OS Proxmox<br>(nvme0n1 - 512Go)"]:::osNode
            CACHE1["Cache Ceph DB/WAL<br>(nvme1n1 - 512Go)"]:::cephCache
            DATA1["Donn√©es Ceph<br>(RAID5 HDD md0 - 18To)"]:::cephData
        end

        subgraph S2 ["Stockage Node-02"]
            OS2["OS Proxmox<br>(nvme0n1 - 512Go)"]:::osNode
            CACHE2["Cache Ceph DB/WAL<br>(nvme1n1 - 512Go)"]:::cephCache
            DATA2["Donn√©es Ceph<br>(RAID5 HDD md0 - 18To)"]:::cephData
        end

        subgraph S3 ["Stockage Node-03"]
            OS3["OS Proxmox<br>(nvme0n1 - 512Go)"]:::osNode
            CACHE3["Cache Ceph DB/WAL<br>(nvme1n1 - 512Go)"]:::cephCache
            DATA3["Donn√©es Ceph<br>(RAID5 HDD md0 - 18To)"]:::cephData
        end
        
        Pool(("Pool Ceph Distribu√©<br>'Stockage-HA'<br>R√©plication x3 <br> (Capacit√©: ~17 To utiles)")):::cephData
    end

    N1 --- OS1
    N2 --- OS2
    N3 --- OS3

    CACHE1 & DATA1 -.- Pool
    CACHE2 & DATA2 -.- Pool
    CACHE3 & DATA3 -.- Pool
```
